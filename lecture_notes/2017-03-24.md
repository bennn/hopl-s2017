## Title: Type-directed Compilation, Part I
### Speaker: Leif Andersen

Today is the story of how 2 worlds came together:

1. World of compiler hackers, C and Fortran programmers, "types go fast" people
2. World of type theorists, polymorphism and parametricity, "more types = faster"

> "more types therefore faster" was an unsubstantiated claim until 1992


### _Unboxed Objects and Polymorphic Typing_, Xavier Leroy, POPL 1992

target language: OCaml (ML light)
problem: the compiler used boxed data
goal: use the type system to implement an "unboxing" compiler pass;
      instead of representing an integer as a pointer to a machine number,
      just use the machine number (need to be careful with floats ---
      they require 2 words of space)

> M: why do we care about this?

> X: cache hits

> L: paper claims even the constant-factor lookup is a problem


### _Compiling Polymorphism Using Intensional Type Analysis_, Robert Harper and Greg Morrisett, POPL 1995

> why are they breaking their tongues calling it "intensional type analysis"?
> because the proper word is _ad-hoc polymorphism_, but everyone knows ad-hoc
> polymorphism is "types at runtime" is BAD. It's marketing.

> J: why not use templates?

> S: templates don't have a clear / clean design

> L: templates make too much code

> M: templates work by **recompiling** code. There's no separate compilation!


Leroy's 1992 paper had a mistake in the proof (related to effects).

> A: what's the bug?

> M: his transformation is moving / shifting side effects

> M: Harper's insight was that _let polymorphism_ is a bad idea, Hindley-Milner
>    was already buggy in the same sense.

> R: LP wihout value restriction

Key idea (fixed proof) is to compile `Λ` to `λτ` and everything works.
But this requires types at runtime.

> M: you should all read Cardelli's _Typeful Programming_
>    also don't forget the Bruce Duba metatheorem, that your calculus needs at
>    least 2 effects otherwise there is no hope it will scale to a real language


### _TIL: A Type-Directed Optimizing Compiler for ML_, David Tarditi and Greg Morrisett and Perry Cheng and Chris Stone and Robert Harper and Peter Lee, PLDI 1996

nice contrast to previous paper, keep types for a long time in the compiler,
but erase for runtime

relation between source-language and intermediate-language types,
add environments after each pass to preserve/check the relation

> A: are source/target related by a **relation** or a **function** ?

> ?: relation would be unsound

> R: does TIL have optional optimizations?


### _From System F to Typed Assembly Language_, Greg Morrisett and David Walker and Karl Crary and Neal Glew, POPL 1998

Two rules of TAL:

- do not read uninitialized memory
- do not allocate memory you do not own

> M: these rule out standard Fortran optimizations, e.g. the fast way to
>    create an array with millions of cells


> ?: this is not assembly, this is fake C without function pointers

> D: it's assembly with a garbage collector

> ?: Reynolds suggested TAL to Peter Lee and Bob Harper

meta theorem: for all X, Reynolds thought of X years ago

- - -

> M: Leroy gave a keynote later at TLDI, admitted type directed compilation was
>    a failure in OCaml. That was the turning point, when he started work on
>    CompCert. The message today is, on occasion you need to assess whether
>    you want to continue spending brainpower on a particular problem. Maybe
>    yes, maybe no, but you need to step back and assess.


> M: I have discussed this with Xavier occasionally and I think that besides what
>    worked "for OCaml", there is a larger question of whether "type-directed
>    compilation" matters at all. To my knowledge, there is no production language
>    today that can substantiate the claim that keeping types all along is
>    instrumental to efficiency of the generated code. (GHC Core demonstrates that
>    having a typed intermediate language is beneficial for language design and
>    compiler testing, and the Rust people claim that their type system, giving
>    strong aliasing guarantee, will allow better optimizations than C/C++, but
>    currently they rely on LLVM and haven't implemented optimizations taking
>    noticeable advantage of this.). Even if you lose types, you can reconstruct a
>    lot locally by static analysis of the code (surely this is how good Scheme
>    implementations do it), and the aspects of typing that are relevant to
>    optimization (mostly data representation questions) are much simpler than
>    surface-language systems. Combined with inlining, local analyses can give you
>    90% of the optimization potential at lower effort. What you lose by forgetting
>    the types is the global information across module boundaries; you can regain
>    that by having optimization summaries, and in effect the C compiler community,
>    with their current obsession on (Thin)LTO (Link-Time Optimization), is
>    rediscovering that types/summaries give you incrementality/modularity and thus
>    scalability.

> G: My personal opinion is that keeping types all the way certainly is a nice
>    compiler design and I would be willing to push making more of those static
>    analyses closer to type systems -- but we should recognize that this remains,
>    in effect, a minority opinion. Sam Lindley recently pointed out to me a benefit
>    of un-typed compilers that I had not previously considered: it is much easier
>    for writers of experimental languages (eg. Coq, Links, Frank etc.) to target
>    the OCaml backend than GHC's, because they don't have to reconstruct a typing
>    derivation in a type system that is different and maybe weaker than theirs.
